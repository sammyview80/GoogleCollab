{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsgjSZzl0hv8"
      },
      "source": [
        "<div align=\"center\" dir=\"auto\">\n",
        "<p dir=\"auto\">\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/write-with-neurl/modelbit-articles/blob/main/modelbit-08/Deploy_Llama_2_7b_Text_Summarization_LangChain_Modelbit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6iGdnxPBU4W"
      },
      "source": [
        "# ‚ö°Deploy Llama 2-7B ü¶ô as a REST Endpoint with Langchain ü¶úüîó and Modelbit üü™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rZkemV-B7--"
      },
      "source": [
        "## üßë‚Äçüíª Installations and Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ZzIy2-eFXl"
      },
      "source": [
        "This walkthrough will guide you through deploying a Llama 2-7B ü¶ô directly from this notebook using Modelbit.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ERXmDHWT-S"
      },
      "source": [
        "### üì• Install Pre-requisite Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibae9vpj0tNY"
      },
      "source": [
        "To set up your environment, install the following packages:\n",
        "- LangChain - `langchain==0.0.335`\n",
        "- Modelbit - `modelbit==0.30.13`\n",
        "- Huggingface_hub - `huggingface-hub==0.19.1`\n",
        "- LLaMA_cpp_python - `llama_cpp_python==0.2.17`\n",
        "\n",
        "As of this writing, those are the specific versions the following `pip` command installs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob863tSOeHuP",
        "outputId": "c7db11d1-acdf-4beb-a17f-67d2f02d6226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.8-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Collecting modelbit\n",
            "  Downloading modelbit-0.34.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.21 (from langchain)\n",
            "  Downloading langchain_community-0.0.21-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.24 (from langchain)\n",
            "  Downloading langchain_core-0.1.24-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m241.3/241.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
            "  Downloading langsmith-0.1.3-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Collecting pycryptodomex (from modelbit)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from modelbit) (1.5.3)\n",
            "Collecting types-requests (from modelbit)\n",
            "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl (14 kB)\n",
            "Collecting types-PyYAML (from modelbit)\n",
            "  Downloading types_PyYAML-6.0.12.12-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from modelbit) (3.1.3)\n",
            "Collecting types-pkg-resources (from modelbit)\n",
            "  Downloading types_pkg_resources-0.1.3-py2.py3-none-any.whl (4.8 kB)\n",
            "Collecting zstandard (from modelbit)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from modelbit) (1.4.4)\n",
            "Collecting texttable (from modelbit)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from modelbit) (1.0.3)\n",
            "Collecting pkginfo (from modelbit)\n",
            "  Downloading pkginfo-1.9.6-py3-none-any.whl (30 kB)\n",
            "Collecting boto3>=1.23.0 (from modelbit)\n",
            "  Downloading boto3-1.34.45-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<2,>=1.21.1 (from modelbit)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting build (from modelbit)\n",
            "  Downloading build-0.10.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from modelbit) (7.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting botocore<1.35.0,>=1.34.45 (from boto3>=1.23.0->modelbit)\n",
            "  Downloading botocore-1.34.45-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.23.0->modelbit)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.23.0->modelbit)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->modelbit) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->modelbit) (2.0.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.24->langchain) (3.7.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->modelbit) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->modelbit) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->modelbit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->modelbit) (2023.4)\n",
            "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting types-requests (from modelbit)\n",
            "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.20240106-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.20231231-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.9-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.8-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.7-py3-none-any.whl (14 kB)\n",
            "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
            "Collecting types-urllib3 (from types-requests->modelbit)\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->modelbit) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: types-urllib3, types-PyYAML, types-pkg-resources, texttable, zstandard, urllib3, types-requests, pycryptodomex, pkginfo, mypy-extensions, marshmallow, jsonpointer, jmespath, typing-inspect, jsonpatch, build, botocore, s3transfer, langsmith, dataclasses-json, langchain-core, boto3, modelbit, langchain-community, langchain\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: build\n",
            "    Found existing installation: build 1.0.3\n",
            "    Uninstalling build-1.0.3:\n",
            "      Successfully uninstalled build-1.0.3\n",
            "Successfully installed boto3-1.34.45 botocore-1.34.45 build-0.10.0 dataclasses-json-0.6.4 jmespath-1.0.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.8 langchain-community-0.0.21 langchain-core-0.1.24 langsmith-0.1.3 marshmallow-3.20.2 modelbit-0.34.0 mypy-extensions-1.0.0 pkginfo-1.9.6 pycryptodomex-3.20.0 s3transfer-0.10.0 texttable-1.7.0 types-PyYAML-6.0.12.12 types-pkg-resources-0.1.3 types-requests-2.31.0.6 types-urllib3-1.26.25.14 typing-inspect-0.9.0 urllib3-1.26.18 zstandard-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain huggingface_hub modelbit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9--x9qjDQaJ"
      },
      "source": [
        "### üèóÔ∏è Build  Llama 2-7b ü¶ô with GPU Support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGgzqOxCCaeQ"
      },
      "source": [
        "You will load the embedding model directly onto your GPU device. Set an environment variable `CMAKE_ARGS` with the value `-DLLAMA_CUBLAS=on` to indicate that the `llama_cpp_python` package should be built with [cuBLAS support](https://developer.nvidia.com/cublas). cuBLAS is a GPU-accelerated library provided by NVIDIA as part of their CUDA toolkit, which offers optimized implementations for standard basic linear algebra subprograms.\n",
        "\n",
        "> The `llama_cpp_python` package provides Python bindings for the llama.cpp library that bridges the gap between the C++ codebase of `llama.cpp` and Python to access and use the functionalities of llama.cpp directly from Python scripts.\n",
        "\n",
        "Use the `FORCE_CMAKE=1`` environment variable to force the use of cmake and install the pip package for the Metal support ([source](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-openblas--cublas--clblast))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR4qna2reFXr",
        "outputId": "5a1dd1e6-b289-4089-f6c5-366cd8ca51be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_cpp_python\n",
            "  Downloading llama_cpp_python-0.2.44.tar.gz (36.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_cpp_python) (4.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama_cpp_python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama_cpp_python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama_cpp_python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama_cpp_python) (2.1.5)\n",
            "Building wheels for collected packages: llama_cpp_python\n",
            "  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.2.44-cp310-cp310-manylinux_2_35_x86_64.whl size=2591542 sha256=65e2986358a90649367a87cd3f5c91499bd88b2801cc247a1b3ee41c0fc13780\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f0/52/1716aa7fefc7eb2a9b76775b0a61fc131b7dcc961e310a048a\n",
            "Successfully built llama_cpp_python\n",
            "Installing collected packages: llama_cpp_python\n",
            "Successfully installed llama_cpp_python-0.2.44\n"
          ]
        }
      ],
      "source": [
        "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" # Build llama_cpp_python on NVIDIA cuBLAS for GPU support\n",
        "FORCE_CMAKE=1 # Force installation to use cuBLAS\n",
        "\n",
        "!pip install llama_cpp_python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kih5nWDPV3ng"
      },
      "source": [
        "### üì•ü¶ô Download the Llama-2-7B-GGUF model using the Hugging Face CLI command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMpAsgPkeFXs"
      },
      "source": [
        "Use the HF CLI to download the Llama-2-7B-GGUF model file from a repository in the Hugging Face model Hub and save it to this notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIvoD4qWeFXs",
        "outputId": "3048fac1-b8c0-456b-b637-d1e19c61c7dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_0.gguf to /root/.cache/huggingface/hub/tmp076udjtf\n",
            "llama-2-7b.Q4_0.gguf: 100% 3.83G/3.83G [00:39<00:00, 96.8MB/s]\n",
            "./llama-2-7b.Q4_0.gguf\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli download TheBloke/Llama-2-7B-GGUF llama-2-7b.Q4_0.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk6T8SeDeFXs"
      },
      "source": [
        "## ü¶úüîó Set Up the Prompt Template with LangChain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_xfVsgbXPnO"
      },
      "source": [
        "After downloading the model file, you need to set up a [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/). The prompt template defines the input variables and the response format for the LlamaCpp model.\n",
        "\n",
        "In this case, the input variables are `text` and `num_of_words`, which represent the Twitter thread and the desired length of the summary, respectively.\n",
        "\n",
        "The response format is a `str` that includes the original Twitter thread and the generated summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1QsUfHA7eFXt"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"num_of_words\"],\n",
        "    template=\"\"\"\\n\\n### Instruction:\\nGiven a ubuntu command: '{text}', You are tasked with providing a command for ubuntu terminal in {num_of_words} words and ensure the command doesn't lose the main context of ubuntu. \\n\\n### Response:\\n\"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxfLaoxIYQlE"
      },
      "source": [
        "## ü¶ô‚õìÔ∏è initialize Llama Model Weights and LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SaTaQ2yeFXt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "After crafting your prompt, initialize the LlamaCpp model with the model file path and context size(`n_ctx`).\n",
        "\n",
        "Use the prompt template to create an LLMChain class with the initialized LlamaCpp model and prompt template.\n",
        "\n",
        "Finally, the `load_llm` function is defined to load the LlamaCpp model embeddings/weights and cache it for future use. Then we deifne a function `summarize_thread` that takes a text string and a number of words as input, and generates a summary of the text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aSFWhMCFa1x7"
      },
      "outputs": [],
      "source": [
        "# Importing necessary classes and modules\n",
        "from langchain.chains import LLMChain  # Used for creating language model chains\n",
        "from langchain.callbacks.stdout import StdOutCallbackHandler  # Handles output to standard output\n",
        "import os  # Used for operating system dependent functionality\n",
        "from langchain.llms.llamacpp import LlamaCpp  # Interface for LlamaCpp model\n",
        "from functools import cache  # Used for caching function results\n",
        "\n",
        "# Setting up the file path for the model\n",
        "folder = ''  # Directory where the model file is located (empty if in current directory)\n",
        "file_name = 'llama-2-7b.Q4_0.gguf'  # Name of the model file\n",
        "file_path = os.path.join(folder, file_name)  # Full path to the model file\n",
        "\n",
        "@cache\n",
        "def load_llm():\n",
        "    \"\"\" Loads the LlamaCpp model with a specified model path and context size.\n",
        "      Uses caching to optimize performance by storing the result for subsequent calls.\n",
        "    \"\"\"\n",
        "    llm = LlamaCpp(model_path=file_path, n_ctx=8191,n_gpu_layers=20,\n",
        "    n_batch=20, f16_kv=True)  # Load the LlamaCpp model // context window - 8191 // batch size 20 // gpu layer offload size 20 // F16_kv=True to enable half precision for key value cache\n",
        "\n",
        "    return llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XANPrtnEJVTw"
      },
      "source": [
        "### üìú Define Inference Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCLqvWNXJi-j"
      },
      "source": [
        "This function takes a text string and an optional `num_of_words argument` (defaulting to 200).\n",
        "\n",
        "`llm = load_llm()` - calls the `load_llm` function to get the loaded LlamaCpp model.\n",
        "\n",
        "`chain = LLMChain(llm=llm, prompt=prompt)` - Instantiates an LLMChain object with the LlamaCpp model and a prompt.\n",
        "\n",
        "`return chain.run(...)` - executes the chain with the specified parameters and returns the result. The run method is passed the text and number of words, along with a callback handler `(StdOutCallbackHandler())` which  handles the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K7tHAyV_JVzQ"
      },
      "outputs": [],
      "source": [
        "def summarize_thread(text: str, num_of_words: int = 200):\n",
        "    \"\"\" Summarizes the given text using the LlamaCpp model.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be summarized.\n",
        "        num_of_words (int, optional): Number of words for the summary. Defaults to 200.\n",
        "\n",
        "    Returns:\n",
        "        The summary of the text.\n",
        "    \"\"\"\n",
        "    llm = load_llm()  # Load the LlamaCpp model\n",
        "    # Initialize LLMChain with the LlamaCpp model and the provided text as prompt\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    # Run the chain with the text and word limit, and return the result\n",
        "    return chain.run({\"text\": text, \"num_of_words\":num_of_words}, callbacks=[StdOutCallbackHandler()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceCcSkZneFXu"
      },
      "source": [
        "## üß™ Test the Inference Function with a Sample Twitter Thread üßµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "sCeuD0yqeFXu",
        "outputId": "d6f0f0f8-cd30-4f1f-9e22-5eef636f019c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "### Instruction:\n",
            "Given a ubuntu command: 'test me', You are tasked with providing a command for ubuntu terminal in 200 words and ensure the command doesn't lose the main context of ubuntu. \n",
            "\n",
            "### Response:\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16101.98 ms\n",
            "llama_print_timings:      sample time =     145.70 ms /   256 runs   (    0.57 ms per token,  1757.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =   26987.00 ms /    42 tokens (  642.55 ms per token,     1.56 tokens per second)\n",
            "llama_print_timings:        eval time =  195704.03 ms /   255 runs   (  767.47 ms per token,     1.30 tokens per second)\n",
            "llama_print_timings:       total time =  223772.53 ms /   297 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The instruction was to provide a command for Ubuntu Terminal, and the command should not lose its main context. As the first word, I believe it is not necessary to explain what is Ubuntu Terminal, so I think it is not necessary to explain it, but it should be introduced that it is a shell, which is used in Ubuntu. In my opinion, the commands should be simple, so I hope I can write something simple, but not too simple that it does not contain any meaningful commands.\\n\\n### Sample Output\\n```\\n> test me\\n```\\n\\n### My Solution\\n\\n#### Instructions\\n\\n##### Explain the solution and how it works\\nI think that it should be simple, so I just use the basic commands like \"cat\" and \"echo\" but I don\\'t think it is too simple because I tried to introduce some commands that I think is useful, like \"find\", \"clear\", \"sleep\", \"read\" etc...\\n\\n##### How it works? Explain your solution(s) to the problem and how it(they) solves(s) the problem.\\nI think it is simple, but it works because I tried to make the commands that I write simple but also useful,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "text=\"\"\"reset wifi command ubuntu\"\"\"\n",
        "\n",
        "# call inference function\n",
        "summarize_thread(text='test me', num_of_words=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1IAVPthtIFf"
      },
      "source": [
        "## üöÄ Deploying Llama 2-7B ü¶ô as a REST API Endpoint with Modelbit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzynYLE_eFXr"
      },
      "source": [
        "\n",
        "\n",
        "Modelbit is a lightweight platform designed to make deploying any ML model to a production endpoint fast and simple. With the ability to [deploy models from anywhere](https://www.modelbit.com/product/deploy-from-anywhere), it makes deploying your custom ML model as simple as passing an inference function to ‚Äúmodelbit.deploy()‚Äù.\n",
        "\n",
        "Here are the basics you need to know about Modelbit:\n",
        "- **Deployment from any Python environment:** Models can be deployed directly from Jupyter Notebooks, Hex, Deepnote, and VS Code.\n",
        "- **Dependency detection:** Automatically detects which dependencies, libraries, and data your model needs and includes them in your model‚Äôs production Docker container.\n",
        "- **REST API Endpoints:** Your models will be callable as [RESTful API endpoints](https://doc.modelbit.com/endpoints/).\n",
        "- **Git-based version control:** Track and manage model iterations with Git repositories.\n",
        "- **CI/CD integration:** Seamlessly integrate model updates and deployment into continuous integration and continuous delivery (CI/CD [link text](https://doc.modelbit.com/git/#connect-your-own-github-gitlab-or-azure-devops-repo)) pipelines like [GitHub Actions](https://github.com/features/actions) and [GitLab CI/CD](https://about.gitlab.com/solutions/continuous-integration/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpha0sX2s37a"
      },
      "source": [
        "## üîí Log into Modelbit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj09lleau_Fi"
      },
      "source": [
        "Modelbit integrates with version control and exepriment iteration tools like gitlab, github, Weights & Biases and neptune.ai to move models from development to production quickly.\n",
        "\n",
        " To get started;\n",
        "\n",
        "Create an account with [Modelbit]('modelbit.com') if you haven't already.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ERlBwMHzeFXr"
      },
      "outputs": [],
      "source": [
        "import modelbit\n",
        "\n",
        "# Log into the 'modelbit' service using the development (\"dev\") branch\n",
        "# Ensure you create a \"dev\" branch in Modelbit or use the \"main\" branch for your deployment\n",
        "mb = modelbit.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsBborOUUogX"
      },
      "source": [
        "## ‚ö° Time to `modelbit.deploy()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnyH-TT_eFXv"
      },
      "source": [
        "Deploy the `summarize_thread` function to Modelbit with the [`mb.deploy()`](https://doc.modelbit.com/api-reference/deploy/) API.\n",
        "\n",
        "This API takes the inference function as an argument and deploys it to Modelbit, making it available through a REST API.\n",
        "\n",
        "Once deployed, you can use the REST API to call the function and generate summaries of Twitter threads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "wRZYdHm0eFXv",
        "outputId": "a11c4591-220b-449d-c38c-be0baea9e6c0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "  <div>\n",
              "    <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Deploying </span> <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">summarize_thread</span>\n",
              "  </div>\n",
              "  \n",
              "  \n",
              "\n",
              "\n",
              "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; margin-top: 10px;\">\n",
              "    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #845B99;\">Heads up!</div>\n",
              "    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\" id=\"fst-mb-799393689\">\n",
              "      <div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; border-left: 1px solid #845B99; margin-bottom: 10px;\">\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "        Your deployment specifies <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">llama_cpp_python==0.2.17</span> but that version was not\n",
              "        found on PyPI. Is the version correct?\n",
              "      </div>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "  </div>\n",
              "    </div>\n",
              "    \n",
              "      \n",
              "        <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\" id=\"btn-mb-799393689\">\n",
              "          There are 2 more inconsistencies.\n",
              "          <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; text-decoration: underline; cursor: pointer;\" onClick=\"document.getElementById('lst-mb-799393689').style.display='block'; document.getElementById('btn-mb-799393689').style.display='none'; document.getElementById('fst-mb-799393689').style.display='none';\">\n",
              "            View all.\n",
              "          </span>\n",
              "        </div>\n",
              "      \n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; display: none;\" id=\"lst-mb-799393689\">\n",
              "          \n",
              "          <div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; border-left: 1px solid #845B99; margin-bottom: 10px;\">\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "        Your deployment specifies <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">llama_cpp_python==0.2.17</span> but that version was not\n",
              "        found on PyPI. Is the version correct?\n",
              "      </div>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "  </div>\n",
              "          \n",
              "          <div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; border-left: 1px solid #845B99; margin-bottom: 10px;\">\n",
              "\n",
              "    \n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "        You chose <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">langchain==0.0.335</span> for your production environment,\n",
              "        but you have <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">langchain==0.1.8</span> locally.\n",
              "      </div>\n",
              "\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">To match your environment to production, run:</div>\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; padding-left: 15px;\">\n",
              "        <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">!pip install langchain==0.0.335</span>\n",
              "      </div>\n",
              "\n",
              "\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">To match production to your local environment, include this line in your deployment:</div>\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; padding-left: 15px;\">\n",
              "        <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">mb.deploy(my_deploy_function, <b>python_packages=[\"langchain==0.1.8\"]</b>)</span>\n",
              "      </div>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "  </div>\n",
              "          \n",
              "          <div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; border-left: 1px solid #845B99; margin-bottom: 10px;\">\n",
              "\n",
              "    \n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "        You chose <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">llama_cpp_python==0.2.17</span> for your production environment,\n",
              "        but you have <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">llama_cpp_python==0.2.44</span> locally.\n",
              "      </div>\n",
              "\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">To match your environment to production, run:</div>\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; padding-left: 15px;\">\n",
              "        <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">!pip install llama_cpp_python==0.2.17</span>\n",
              "      </div>\n",
              "\n",
              "\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">To match production to your local environment, include this line in your deployment:</div>\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; padding-left: 15px;\">\n",
              "        <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">mb.deploy(my_deploy_function, <b>python_packages=[\"llama_cpp_python==0.2.44\"]</b>)</span>\n",
              "      </div>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "  </div>\n",
              "        \n",
              "      </div>\n",
              "    \n",
              "  </div>\n",
              "\n",
              "  \n",
              "\n",
              "\n",
              "  \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">Uploading dependencies...</div>\n",
              "</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading 'prompt': 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 432/432 [00:00<00:00, 598B/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Success!</div>\n",
              "  \n",
              "    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "      Deployment <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">summarize_thread</span>\n",
              "      will be ready in  a couple minutes.\n",
              "    </div>\n",
              "  \n",
              "\n",
              "  <a href=\"https://app.modelbit.com/w/nupipay/main/deployments/summarize_thread/apis\" target=\"_blank\" style=\"display: inline-block; margin-top: 12px;\" >\n",
              "    <div\n",
              "      style=\"display: inline-block; background-color: #845B99; border-radius: 0.375rem; color: white; cursor: pointer; font-size: 14px; font-weight: 700; padding: 8px 16px;\"\n",
              "      onmouseenter=\"this.style.background='#714488'\"\n",
              "      onmouseleave=\"this.style.background='#845B99'\"\n",
              "    >\n",
              "      View in Modelbit\n",
              "    </div>\n",
              "  </a>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "mb.deploy(summarize_thread, extra_files=[\"llama-2-7b.Q4_0.gguf\"], python_packages=[\"langchain==0.0.335\",\"llama_cpp_python==0.2.17\"],\n",
        "          require_gpu=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4GHndjYkb8T"
      },
      "source": [
        "## üßë‚Äçüç≥ Test the Llama 2-7B ü¶ô REST API Endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5T0nl6keFXv"
      },
      "source": [
        "\n",
        "\n",
        "To test your REST Endpoint, you can use the requests package to send single or batch requests to the API.\n",
        "\n",
        "You can use the `requests.post()` method to send a POST request to the API and use the json module to format the response. Here's an example of how to test your REST Endpoint using Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzLGcEyin6qT"
      },
      "outputs": [],
      "source": [
        "twitter_thread=\"\"\"In today's fast-paced world, it's crucial to stay connected and informed. Communication has evolved over the years, with technology playing a pivotal role. We're now able to share ideas, news, and personal stories with a global audience instantly.\n",
        "\n",
        "The rise of social media has been a game-changer.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-SHt_4Bn2CN"
      },
      "source": [
        "Test your endpoint from the command line using:\n",
        "\n",
        "> ‚ö†Ô∏è Replace the `ENTER_WORKSPACE_NAME` placeholder with your workspace name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhgiteZBeFXw",
        "outputId": "d75eecd7-92f6-457c-dac3-90768c888116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"error\": \"Runtime exited unexpectedly.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Testing the endpoint\"\"\"\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Replace ‚ÄúENTER_WORKSPACE_NAME‚Äù with your Modelbit username\n",
        "# Also change the version branch from `v1` if you have a different deployment version\n",
        "\n",
        "url = \"https://nupipay.app.modelbit.com/v1/summarize_thread/latest\"\n",
        "\n",
        "\n",
        "headers = {\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "data = {\n",
        "    \"data\": ['shutdown command', 200] # text and num_of_words argument for inf function\n",
        "}\n",
        "\n",
        "\n",
        "# POST your request to the endpoint\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "response_json = response.json()\n",
        "\n",
        "print(json.dumps(response_json, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT4lr1ccvD6T"
      },
      "source": [
        "** Use `cURL` **\n",
        "\n",
        "> ‚ö†Ô∏è Replace the `ENTER_WORKSPACE_NAME` placeholder with your workspace name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOSd6vjvoFgw"
      },
      "outputs": [],
      "source": [
        "!curl -s -XPOST \"https://ENTER_WORSKPACE_NAME.app.modelbit.com/v1/resnet_inference/dev/latest\" -d '{\"data\": [twitter_thread, 200]}' | json_pp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Co2SbvLn7qX"
      },
      "source": [
        "# üìö Modelbit Machine Learning Blog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KllWgn6BoCwK"
      },
      "source": [
        "Enjoyed this walkthrough? Check out our blog for more machine learning tutorials.\n",
        "\n",
        "Recommendation:\n",
        "\n",
        "- [Deploying a BERT Model to a REST API Endpoint for Text Classification](https://www.modelbit.com/blog/deploying-a-bert-model-to-a-rest-api-endpoint-for-text-classification)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}